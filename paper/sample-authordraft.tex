%! Author = ashutosh
%! Date = 4/20/22

\documentclass[english]{sobraep}
\usepackage[english]{babel}
\usepackage{float}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    citecolor=green,
    linkcolor=blue,
    filecolor=cyan,
    urlcolor=magenta,
    }
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
\usepackage{listings}


\title{Investigating Structure of Bias Manifold and Bias Evolution over years}


\author{Ashutosh Tiwari\\
	\normalsize ashutiwa@iu.edu
}

\pagestyle{fancy}
\fancyhf{}
\lhead{Presented as a project for  MGMT ACCESS USE BIG DATA(FALL 2022)}
\rfoot{Page \thepage}
\begin{document}
\maketitle



\section{Background}

Bias is an inseparable part of output models of considerable size. This bias is generally the result of data itself on which model is trained. This bias is tackled in general at three different stages in previous attempts. Some works (example ~\cite{DBLP:journals/corr/BolukbasiCZSK16a}) suggest that probably we should try to fix this bias after the model is trained and then debias the generated model embeddings. There is one more approach to learn models  that are fair (free from accumulated bias in dataset). This requires having two "unfair" models and then training a new model in presence of an adversercial setting~\cite{kenna_using_2021}. However we think that all these methods are bandaid at best, because of these reasons \begin{itemize}
    \item These limit usability and scope of a dataset and set of modeling techniques that can be applied to same. Because both of these rely on first training the model, these are methods are not generic enough both to be able to train any model on the dataset.
    \item These are very inefficient in terms of computing needs because first they need to train the model and then remove bias from embeddings/model.

\end{itemize}

Therefore a third method is to rather remove biases from the dataset itself ~\cite{ravfogel_null_2020}. In this project our aim is to figure out the structure of different kinds of biases in datasets and that will hopefull help us figure out different metrics and methods to remove those from the dataset.



\noindent \textbf{Keywords.} Graph Embeddings, Dataset Creation, Learning Fairness, Measuring Bias


\section{Introduction}

As part of this project, I will be training different simple models (Glove, Word2vec) models on different datasets and then will try to see how the structure of bias evolves overtime in each of those cases. To measure bias I will be using WEAT (Word Embedding Association Test) which was introduced in ~\cite{caliskan_semantics_2017}. This uses the evaluation dataset I found here in this \href{https://github.com/chadaeun/weat_replication/tree/master/weat}{github repository}. However because this dataset is pretty small, depending on time I will see I can find other evaluation datasets as well.

On top of this I will try to evaluate this general assumption in most of the previous works that in general bias manifolds is general. For this purpose I will try to train all these different models and see if I can separate gender bias using different methods used for linear separability. For this I will be using the dataset used by ~\cite{garg_word_2018}.

Also sometime earlier before I took this course I implemented python version for ~\cite{brunet_understanding_2019} using this \href{https://github.com/mebrunet/understanding-bias}{official repository} to calculate bias gradient for a document given a glove model and cooccurance matrix of same. I hope that I will have time to use this to figure out most biased documents in datasets I use for this project. This will help us determine if bias learned by glove models can be verified by human annotators.




\section{Methodology}

I first divide the project into two parts. These are listed below:

\begin{itemize}
    \item \textbf{Part 1:} In this part I will be training different models on different datasets and then will try to see how extend of bias increases or decreases over time.
    \item \textbf{Part 2:} This part to is to verify or see if (gender) bias manifold itself is itself linear or not in nature.

\end{itemize}

Different models I will be using for this project are listed below:

\begin{itemize}
    \item Google pretrained word2vec 300 dimensional \href{
https://www.kaggle.com/datasets/leadbest/googlenewsvectorsnegative300}{model}
    \item Word2vec (implemented using Gensim)
    \item Glove (implemented taken from \href{https://github.com/stanfordnlp/GloVe}{here}, however wrapper on top of this C++ code is written in python)
\end{itemize}
\section{Results}

\section{Discussion}

\section{Conclusion}

\section{Future Work}


\begin{acks}
\end{acks}



\bibliographystyle{alpha}

\bibliography{sample-base}

\end{document}
\endinput